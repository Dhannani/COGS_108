{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COGS 108 - Final Project\n",
    "\n",
    "### Important\n",
    "- ONE, and only one, member of your group should upload this notebook to TritonED.\n",
    "- Each member of the group will receive the same grade on this assignment.\n",
    "- Keep the file name the same: submit the file 'FinalProject.ipynb'.\n",
    "- Only upload the .ipynb file to TED, do not upload any associted data. Make sure that \n",
    "  for cells in which you want graders to see output that these cells have been executed.\n",
    "\n",
    "### Group Members: Fill in the Student IDs of each group member here\n",
    "Replace the lines below to list each persons full student ID, ucsd email and full name.\n",
    "\n",
    "- \n",
    "- \n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "Below are specific third-party libraries we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob\n",
    "!{sys.executable} -m pip install gender-guesser\n",
    "!{sys.executable} -m pip install selenium\n",
    "!{sys.executable} -m pip install pillow\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "%matplotlib inline\n",
    "\n",
    "##These Dependencies Need To be Downloaded\n",
    "from textblob import TextBlob \n",
    "import gender_guesser.detector as gender\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate My Professor (RMP) Scraping\n",
    "#### Scraping Rate My Professor for All UCSD Professors\n",
    "To scrape data from RMP we first need to figure out the GET request URL's that happen in the background when we use the site. Once we have those, we can get a list of all professors at UCSD <em>getProfessorList</em> and extract the ids of those professors. Using the teacher ids(tids) we can then query all the reviews for that professor <em>getProfessorInformation</em> and save it to a dataframe. <em>generateAllProfInformation</em> does this for all professors and save it to a dataframe along with some extra professor metadata that was returned with the list of all professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProfessorList(schoolID):\n",
    "    page_id = 1\n",
    "    professorList = []\n",
    "    while True:\n",
    "        page = requests.get(\"http://www.ratemyprofessors.com/filter/professor/?&page=\" \n",
    "                            + str(page_id) + \"&filter=teacherlastname_sort_s+asc&query=*%3A*&queryoption=TEACHER&queryBy=schoolId&sid=\" \n",
    "                            + str(schoolID))\n",
    "        \n",
    "        jsonpage = json.loads(page.content)\n",
    "        professors = jsonpage['professors']\n",
    "        professorList.extend(professors)\n",
    "        \n",
    "        if(int(jsonpage['remaining']) == 0):\n",
    "            break\n",
    "        else:\n",
    "            page_id += 1\n",
    "             \n",
    "    \n",
    "    df = pd.DataFrame(professorList)\n",
    "    df = df.drop(df[df['tNumRatings'] == 0].index) #drop rows without responses\n",
    "    df.to_json('professors.json')\n",
    "\n",
    "    '''\n",
    "    #save to json file\n",
    "    with open('professors.json', 'w') as outfile:\n",
    "        json.dump(professorList, outfile)\n",
    "    '''\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def getProfessorInformation(tid):\n",
    "    page_id=1\n",
    "    pages = []\n",
    "    while True:\n",
    "        url = 'http://www.ratemyprofessors.com/paginate/professors/ratings?tid='+ str(tid)+'&filter=&courseCode=&page='+str(page_id)\n",
    "        page = requests.get(url);\n",
    "        r_json = json.loads(page.content)\n",
    "        #page_of_comments = pd.DataFrame.from_dict(r_json['ratings'], orient='columns')\n",
    "        pages.extend(r_json['ratings'])\n",
    "        \n",
    "        if(int(r_json['remaining']) == 0):\n",
    "            break\n",
    "        else:\n",
    "            page_id += 1\n",
    "        \n",
    "    df = pd.DataFrame(pages)\n",
    "    prof = pList.loc[pList['tid'] == tid]\n",
    "    df.insert(0,'tDept',prof['tDept'].values[0])\n",
    "    df.insert(0,'tFname',prof['tFname'].values[0])\n",
    "    df.insert(0,'tLname',prof['tLname'].values[0])\n",
    "    df.insert(0,'tid',tid)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generateAllProfInformation():\n",
    "    data = []\n",
    "    tids = pList['tid'].values\n",
    "    \n",
    "    for i in tids:\n",
    "        data.append(getProfessorInformation(i))\n",
    "    \n",
    "    data = pd.concat(data)\n",
    "    data.to_csv(\"profData.csv\",index=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First We Grab All The Professors Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucsdID = 1079\n",
    "df_professors = getProfessorList(ucsdID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next We Scrape All of Their Reviews\n",
    "** Note This Will Take A While"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses = generateAllProfInformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Let's Augment this Data for Better Analysis\n",
    "#### Adding Gender\n",
    "Since Rate My Professor doesn't provide any information on gender, we needed to way to obtain the gender some other way. Fortunately we do have name data, and for the most part we an deduce gender from common names. With this intuition we found a python model that does exactly this and the following code attempts to classify a professor by gender using their first name\n",
    "\n",
    "#### Create id to gender dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_professors['tFname'].values\n",
    "tids = df_professors['tid'].values\n",
    "\n",
    "gender_model = gender.Detector(case_sensitive=False)\n",
    "genders = {}\n",
    "u = 0\n",
    "\n",
    "for i,v in enumerate(names):\n",
    "    name = v.split(' ')[0]\n",
    "    g = gender_model.get_gender(name)\n",
    "    if g == 'male' or g == 'mostly_male':\n",
    "        genders[tids[i]] = 'M'\n",
    "    elif g == 'female' or g =='mostly_female':\n",
    "        genders[tids[i]] = 'F'\n",
    "    elif g == 'unknown' or g == 'andy':\n",
    "        genders[tids[i]] = 'U'\n",
    "        u+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** An important note is that the gender data produced is only as good as the model, and we are aware that this may affect our overall analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add genders to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert dummy column\n",
    "df_responses.insert(4,'gender','M')\n",
    "\n",
    "for k,v in genders.items():\n",
    "    index = df_responses[df_responses['tid'] == str(k)].index\n",
    "    df_responses.loc[index,'gender'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Sentiment from RMP Comments\n",
    "The majority of the data from RMP comes in the form of comments, which are just long strings. To be able to analyze this data numerically, we obtained the sentiment value of each comment. To do this we used a common python model TextBlob, which allowed us to simply plug in comments to generate sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments are generally dirty, containing punctuation and numbers which doesn't help in determining sentiment. The following functions cleans up the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(comment): \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", comment).split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment values take on values between -1 and 1, which refer to negative and positive sentiments, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_sentiment(comment): \n",
    "    analysis = TextBlob(clean_comment(comment)) \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive', analysis.sentiment.polarity\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral', analysis.sentiment.polarity\n",
    "    else: \n",
    "        return 'negative', analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert dummy columns\n",
    "df_responses.insert(18,'sentimentValue',0)\n",
    "df_responses.insert(18,'sentiment','positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_responses.index:\n",
    "    comment = df_responses.loc[i,'rComments']\n",
    "    if(not pd.isna(comment)):\n",
    "        sentiment,polarity = get_comment_sentiment(comment)\n",
    "        df_responses.loc[i,'sentiment'] = sentiment\n",
    "        df_responses.loc[i,'sentimentValue'] = polarity\n",
    "    else:\n",
    "        df_responses.loc[i,'sentiment'] = 'N/A'\n",
    "        df_responses.loc[i,'sentimentValue'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have some useful data, let's clean up what we don't need and standardize the columns we want to keep\n",
    " Remove Columns that are not Useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropColumns = ['rOverallString', 'onlineClass', 'rErrorMsg', 'rStatus', 'teacher', 'unUsefulGrouping', 'usefulGrouping', 'easyColor', 'helpColor', 'clarityColor']\n",
    "df_responses.drop(columns=dropColumns,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize Useful Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yesNoToInt(str_in):\n",
    "    if(str_in == \"Yes\"):\n",
    "        return 1\n",
    "    elif (str_in == \"No\"):\n",
    "        return 0\n",
    "    return str_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToGPA(str_in):\n",
    "    gpaDict = {\n",
    "        'A+': 4.0,\n",
    "        'A' : 4.0,\n",
    "        'A-': 3.7,\n",
    "        'B+': 3.3,\n",
    "        'B' : 3.0,\n",
    "        'B-': 2.7,\n",
    "        'C+': 2.3,\n",
    "        'C' : 2.0,\n",
    "        'C-': 1.7,\n",
    "        'D+': 1.3,\n",
    "        'D' : 1.0,\n",
    "        'D-': 0.7,\n",
    "        'F' : 0.0\n",
    "    }\n",
    "    return gpaDict.get(str_in, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interestToInt(str_in):\n",
    "    if(str_in == \"Low\"):\n",
    "        return 1\n",
    "    elif (str_in == \"Meh\"):\n",
    "        return 2\n",
    "    elif (str_in == \"Sorta interested\"):\n",
    "        return 3\n",
    "    elif (str_in == \"Really into it\"):\n",
    "        return 4\n",
    "    elif (str_in == \"It's my life\"):\n",
    "        return 5\n",
    "    return str_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genderToInt(str_in):\n",
    "    if(str_in == 'M'):\n",
    "        return 1\n",
    "    elif(str_in == 'F'):\n",
    "        return -1\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses[\"rTextBookUse\"] = df_responses[\"rTextBookUse\"].apply(yesNoToInt)\n",
    "df_responses[\"rWouldTakeAgain\"] = df_responses[\"rWouldTakeAgain\"].apply(yesNoToInt)\n",
    "df_responses[\"takenForCredit\"] = df_responses[\"takenForCredit\"].apply(yesNoToInt)\n",
    "df_responses[\"rInterest\"] = df_responses[\"rInterest\"].apply(interestToInt)\n",
    "df_responses[\"gender\"] = df_responses[\"gender\"].apply(genderToInt)\n",
    "df_responses[\"teacherGrade\"] = df_responses[\"teacherGrade\"].apply(letterToGPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.to_csv(\"modifiedProfInfo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a little fun with our new dataset Before Moving On\n",
    "#### We're going to generate a wordcloud of all the comments in RMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(comment for comment in df_responses.rComments if pd.notnull(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100,stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPES Scraping\n",
    "##### Instructions : Download Appropriate Web Driver from the ChromeDrivers folder and add the Executeable to Path\n",
    "https://chromedriver.storage.googleapis.com/index.html?path=74.0.3729.6/\n",
    "\n",
    "Scraping data from CAPES is much more tedious than RMP due to the fact that CAPES data is only accessible to UCSD students. For this reason we need to actually perform an automated login to get to the data. We used Selenium and a Chrome Driver to programically login to capes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Login To Capes\n",
    "The following codes requires you to input a username, and will provide a secure input field for password when run. The code then opens up a Google Chrome browser and performs an automated login to CAPES.\n",
    "\n",
    "** Don't worry, the password is protected and erased as soon as the cell finishes running or potentially crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter credentials\n",
    "username = ''\n",
    "password = getpass.getpass()\n",
    "\n",
    "\n",
    "if(len(username) < 1):\n",
    "    assert len(username) > 0\n",
    "    \n",
    "if(len(password) < 1):  \n",
    "    password = '' #safety\n",
    "    assert len(password) > 0\n",
    "\n",
    "##init chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://cape.ucsd.edu/responses/Results.aspx\")\n",
    "\n",
    "#fill in username\n",
    "elem = driver.find_element_by_name(\"urn:mace:ucsd.edu:sso:username\")\n",
    "elem.clear()\n",
    "elem.send_keys(username)\n",
    "\n",
    "#fill in password\n",
    "elem = driver.find_element_by_name(\"urn:mace:ucsd.edu:sso:password\")\n",
    "elem.clear()\n",
    "elem.send_keys(password)\n",
    "\n",
    "#login!\n",
    "elem = driver.find_element_by_name(\"_eventId_proceed\").click()\n",
    "\n",
    "#reset username & password for safety\n",
    "username = ''\n",
    "password = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all the Departments\n",
    "To be able to query into capes, we need all the department names. Thanfully Selenium has a method to select from the dropdown menu that provides all the deparment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = Select(driver.find_element_by_name('ctl00$ContentPlaceHolder1$ddlDepartments'))\n",
    "options = select.options\n",
    "del options[0] #remove \"select department tag\"\n",
    "departments = [o.get_attribute('value') for o in options]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download All Page Sources For Each Department\n",
    "Using Selenium we download all the webpages source files for each department so that we can use beautiful soup to parse them. Notice we needed to add a timeout since each query has a loading time in CAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basecape = 'http://cape.ucsd.edu/responses/Results.aspx?Name=&CourseNumber='\n",
    "page_sources = []\n",
    "for depts in departments:\n",
    "    req = basecape + depts\n",
    "    driver.get(req)\n",
    "    time.sleep(1)\n",
    "    page_sources.append(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Headers For DataFrame\n",
    "This function builds all the table headers which describe each column in the CAPES tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "table = soup.find('table', attrs={'class':'styled'})\n",
    "for th in table.find('tr').findAll('th'):\n",
    "    headers.append(th.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Table Containing Reviews Into DataFrame\n",
    "Now we use beautiful soup to extract all the data for each department table into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for source in page_sources:\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    table = soup.find('table', attrs={'class':'styled'})\n",
    "    data = []\n",
    "    for i,row in enumerate(table.findAll('tr')):\n",
    "        if i==0:\n",
    "            continue\n",
    "        else:\n",
    "            col_data = []\n",
    "            for td in row.findAll('td'): \n",
    "                col_data.append(td.text.strip())\n",
    "        data.append(col_data)\n",
    "    dataframes.append(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat DataFrame, Add Headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dataframes)\n",
    "df.columns = headers\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing percent with decimal float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPercentage(percent):\n",
    "    percent = percent.strip()\n",
    "    percent = percent.split(' ')[0]\n",
    "    percent = float(percent) / 100.0\n",
    "    return percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing letter grades with purely numerical values for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanGrades(grades):\n",
    "    if(pd.notnull(grades)):\n",
    "        grades.strip()\n",
    "        grades = grades.split(' ')[1]\n",
    "        grades = grades.strip('()')\n",
    "        return float(grades)\n",
    "    else:\n",
    "        return grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we encode the terms by quarter and year so that we can perform time series analysis further on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTerms(terms):\n",
    "    semester = terms[:2]\n",
    "    year     = terms[2:4]\n",
    "    \n",
    "    if(semester == \"WI\"):\n",
    "        return (int)(year+\"0\")\n",
    "    \n",
    "    if(semester == \"SP\"):\n",
    "        return (int)(year+\"1\")\n",
    "    \n",
    "    if(semester == \"S1\" or semester == \"S2\" or semester == \"S3\" or semester == \"SU\"):\n",
    "        return (int)(year+\"2\")\n",
    "    \n",
    "    if(semester == \"FA\"):\n",
    "        return (int)(year+\"3\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following functions are used to make it possible to combine both RMP and CAPES datasets into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the department name from the course description since RMP only has reliable department data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDepartment(course):\n",
    "    course = course.strip()\n",
    "    course = course.split(\" \")[0]\n",
    "    return course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split professors names into first and last to match the convention in RMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFirstName(inst):\n",
    "    if(pd.notnull(inst)):\n",
    "        inst = inst.strip()\n",
    "        inst = inst.split(\",\")[1].strip()\n",
    "        inst = inst.split(\" \")[0].strip()\n",
    "        return inst\n",
    "    else:\n",
    "        return inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitLastName(inst):\n",
    "    if(pd.notnull(inst)):\n",
    "        inst = inst.strip()\n",
    "        inst = inst.split(\",\")[0].strip()\n",
    "        return inst\n",
    "    else:\n",
    "        return inst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval / Enroll\n",
    "You will notice we are using Eval / Enroll as one of the augmented columns. This is because the relationship between these two values is what we will need to correlate RMP and CAPE data as enroll by itself is meaningless without knowing the total possible evals that could have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Rcmnd Class\"] = df[\"Rcmnd Class\"].apply(cleanPercentage)\n",
    "df[\"Rcmnd Instr\"] = df[\"Rcmnd Instr\"].apply(cleanPercentage)\n",
    "df[\"Avg Grade Expected\"] = df[\"Avg Grade Expected\"].apply(cleanGrades)\n",
    "df[\"Avg Grade Received\"] = df[\"Avg Grade Received\"].apply(cleanGrades)\n",
    "df[\"Term\"] = df[\"Term\"].apply(cleanTerms)\n",
    "df[\"tDept\"] = df[\"Course\"].apply(splitDepartment)\n",
    "df[\"tLname\"] = df[\"Instructor\"].apply(splitLastName)\n",
    "df[\"tFname\"] = df[\"Instructor\"].apply(splitFirstName)\n",
    "df[\"Eval / Enroll\"] = df[\"Evals Made\"].values / df[\"Enroll\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Instructor','tLname','tFname','tDept','Course','Term','Enroll','Evals Made', 'Eval / Enroll',\n",
    "'Rcmnd Class', 'Rcmnd Instr', 'Study Hrs/wk', 'Avg Grade Expected', 'Avg Grade Received']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Find Duplicate Names as This Could Cause Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = [(splitFirstName(s) + \" \" + splitLastName(s)) for s in df['Instructor'].unique() if pd.notnull(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = {}\n",
    "dupes = []\n",
    "\n",
    "for x in fn:\n",
    "    if x not in seen:\n",
    "        seen[x] = 1\n",
    "    else:\n",
    "        if seen[x] == 1:\n",
    "            dupes.append(x)\n",
    "        seen[x] += 1\n",
    "        \n",
    "print(\"There are {} professors with the same first and last name, however they are in different departments after further analysis\".format(len(dupes))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below Are the Number of Duplicate Professors In Different Departments\n",
    "Through Individual Analysis we Deduced that the Rest of the 22 Professors were Not Actually Duplicates but Inputted Wrong in CAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = []\n",
    "for d in dupes:\n",
    "    ff = d.split(' ')[0]\n",
    "    ll = d.split(' ')[1]\n",
    "    if(len(df[(df['tLname'] == ll) & (df['tFname'] == ff)]['tDept'].unique()) > 1):\n",
    "        dps.append((ff,ll))\n",
    "display(dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:  Combine RMP and CAPES Data into one Dataframe of All Professors Common to Both Datasets\n",
    "To do this we need a way to find only use the professors that have reviews in RMP and attach those tid's (teacher ids) to the cape dataframe. First let's take a look at those duplicates and import our RMP dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmp = pd.read_csv(open('modifiedProfInfo.csv'), header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the duplicates that are in RMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_dps = []\n",
    "for d in dps:\n",
    "    if(len(df_rmp[(df_rmp['tLname'] == d[1]) & (df_rmp['tFname'] == d[0])]) > 0):\n",
    "        trim_dps.append(d)\n",
    "        \n",
    "trim_dps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point due to the complexity of adding them in and the low number of duplicates we decideds to not to use these professors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now We Will Try to Append the tid's from RMP to CAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tids_nf = []\n",
    "tids = df_rmp['tid'].unique()\n",
    "for tid in tids:\n",
    "    if(pd.notnull(tid)):\n",
    "        rmp_fname = df_rmp[df_rmp['tid'] == tid]['tFname'].unique()[0]\n",
    "        rmp_lname = df_rmp[df_rmp['tid'] == tid]['tLname'].unique()[0]\n",
    "        rmp_fl = (rmp_fname,rmp_lname)\n",
    "        if(rmp_fl not in dps):\n",
    "            indices = df[(df['tLname'] == rmp_lname) & (df['tFname'] == rmp_fname)].index\n",
    "            if(len(indices) > 0):\n",
    "                for i in indices:\n",
    "                    df.loc[i,\"tid\"] = tid\n",
    "            else:\n",
    "                tids_nf.append(tid)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace nan tids with -1 and convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tid'] = df['tid'].fillna(-1)\n",
    "df['tid'] = df['tid'].astype(int)\n",
    "df['tid'] = df['tid'].replace('-1', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs_not_on_RMP = len(df[pd.isnull(df['tid'])]['Instructor'].unique())\n",
    "total_profs_on_CAPES = len(df['Instructor'].unique())\n",
    "print(\"There are {} professors without reviews on RMP, out of the total {} professors on cape\".format(profs_not_on_RMP,total_profs_on_CAPES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the augmentated capes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('capeReviewsCleaned2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Finally Time to Consolidate RMP and CAPES into One DataFrame!\n",
    "Now we want to put together our two dataframes into one, where each row represents a professor. Professors without a tid on RateMyProfessor will have to be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cape = pd.read_csv(\"capeReviewsCleaned2.csv\")\n",
    "rmp = pd.read_csv(\"modifiedProfInfo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the list of columns that our new dataframe will contain, which removes information such as first and last name to follow <strong>safe harbour methods</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnList = ['tid', 'gender', 'tDept', 'Enroll', 'Evals Made', 'Eval/Enroll', 'Rcmnd Class', \n",
    "           'Rcmnd Instr', 'Study Hrs/wk', 'Avg Grade Expected', 'Avg Grade Received', \n",
    "           'rEasy', 'rHelpful', 'rInterest' , 'rOverall', 'rWouldTakeAgain', 'rmp Grade', \n",
    "           'sentimentValue', 'teacherRatingTags', 'rmp Evals/Enroll']\n",
    "\n",
    "df = pd.DataFrame(columns=columnList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating tid list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidList = cape['tid'].unique() # get array of unique tid's\n",
    "tidList = np.setdiff1d(tidList,-1) # remove -1 from array and put in numerical order\n",
    "df['tid'] = tidList #generate rows for tid column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final dataframe will contain rows of professors, therefore there are a few things to consider during the consolidation process\n",
    "\n",
    "1) RMP data will need to be averaged per professor, as each row in the RMP dataframe corresponds to one review per a professor\n",
    "\n",
    "2) CAPE data will need to be averaged by weight, where the weight represents the number of evaluations per class over the total number of evaluations of all classes for that professor (eval/total eval). This is because each row in the CAPE dataframe corresponds to n number of evaluations and should be weighted accordingly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method handles calculating the weighted (eval/total eval) average per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets weighted average of values of a column\n",
    "def getWeightedAvg(col, tid, totEvals):\n",
    "    totEvals = totEvals.values[0]\n",
    "    \n",
    "    for index, row in cape[(pd.isnull(cape[col])) & (cape['tid'] == tid)].iterrows(): #remove evals from total for null entries in col\n",
    "        totEvals -= row['Evals Made']\n",
    "    avg = 0.0\n",
    "    evalsCheck = 0\n",
    "    for index, row in cape[cape['tid'] == tid].iterrows():\n",
    "        if(not pd.isnull(row[col])):\n",
    "            evalsCheck += row['Evals Made']\n",
    "            avg += row[col] * row['Evals Made'] / totEvals # add to weighted avg\n",
    "    \n",
    "    if(evalsCheck != totEvals): #check to see if evals were added correctly\n",
    "        print(\"EVALS CALCULATION ERROR: evalsCheck == \" + evalsCheck + \" , totEvals == \" + totEvals)\n",
    "        \n",
    "    if(avg == 0): #return NaN if nothing was added to avg\n",
    "        return np.nan\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method handles parsing all the teacher tags from RMP and generating a unique csv string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all unique rating tags for a tid\n",
    "def getRatingTags(tid):\n",
    "    tags = []\n",
    "    for index, row in rmp[rmp['tid'] == tid].iterrows():\n",
    "        tg = row['teacherRatingTags']\n",
    "        if(not pd.isnull(tg)):\n",
    "            tg = tg.strip('[ ]')\n",
    "            tg = tg.replace('\\'', ' ')\n",
    "            tg = tg.split(',')\n",
    "            tags.extend([a.strip() for a in tg if (a.strip() not in tags)])\n",
    "    if('' in tags):    \n",
    "        tags.remove('')\n",
    "        \n",
    "    tgstr = \"\"\n",
    "    for tg in tags:\n",
    "        tgstr += tg\n",
    "        tgstr += ','\n",
    "        \n",
    "    return tgstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through each row and add values to each column from either rmp or capes. Since each row represents a unique professor, we must do some data processing on capes and rmp to consolidate multiple entries for each professor, using the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = rmp.tid.value_counts()\n",
    "\n",
    "for tid in tidList:\n",
    "    index = df[df['tid'] == tid].index\n",
    "    #print(tid)\n",
    "    df.loc[index, 'gender'] = rmp[rmp['tid'] == str(tid)]['gender'].values[0]\n",
    "    df.loc[index, 'tDept'] = cape[cape['tid'] == tid]['tDept'].values[0]\n",
    "    df.loc[index, 'Enroll'] = cape[cape['tid'] == tid]['Enroll'].sum()\n",
    "    df.loc[index, 'Evals Made'] = cape[cape['tid'] == tid]['Evals Made'].sum()\n",
    "    df.loc[index, 'Eval/Enroll'] = df[df['tid'] == tid]['Evals Made'] / df[df['tid'] == tid]['Enroll']\n",
    "    df.loc[index, 'Rcmnd Class'] = getWeightedAvg('Rcmnd Class', tid, df[df['tid'] == tid]['Evals Made'])\n",
    "    df.loc[index, 'Rcmnd Instr'] = getWeightedAvg('Rcmnd Instr', tid, df[df['tid'] == tid]['Evals Made'])\n",
    "    df.loc[index, 'Study Hrs/wk'] = getWeightedAvg('Study Hrs/wk', tid, df[df['tid'] == tid]['Evals Made'])\n",
    "    df.loc[index, 'Avg Grade Expected'] = getWeightedAvg('Avg Grade Expected', tid, df[df['tid'] == tid]['Evals Made'])\n",
    "    df.loc[index, 'Avg Grade Received'] = getWeightedAvg('Avg Grade Received', tid, df[df['tid'] == tid]['Evals Made'])\n",
    "    df.loc[index, 'rEasy'] = rmp[rmp['tid'] == str(tid)]['rEasyString'].mean()\n",
    "    df.loc[index, 'rHelpful'] = rmp[rmp['tid'] == str(tid)]['rHelpful'].mean()\n",
    "    df.loc[index, 'rInterest'] = rmp[rmp['tid'] == str(tid)]['rInterest'].mean()\n",
    "    df.loc[index, 'rOverall'] = rmp[rmp['tid'] == str(tid)]['rOverall'].mean()\n",
    "    df.loc[index, 'rWouldTakeAgain'] = rmp[rmp['tid'] == str(tid)]['rWouldTakeAgain'].mean()\n",
    "    df.loc[index, 'rmp Grade'] = rmp[rmp['tid'] == str(tid)]['teacherGrade'].mean()\n",
    "    df.loc[index, 'sentimentValue'] = rmp[rmp['tid'] == str(tid)]['sentimentValue'].mean()\n",
    "    df.loc[index, 'teacherRatingTags'] = getRatingTags(str(tid)) \n",
    "    df.loc[index, 'rmp Evals/Enroll'] = counts.get(str(tid)) / df[df['tid'] == tid]['Enroll']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we have the final dataframe we need to perform analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"FullData.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
